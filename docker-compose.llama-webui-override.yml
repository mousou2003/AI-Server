# Llama WebUI Specific Override
# Usage: docker compose -f docker-compose.ollama.yml -f docker-compose.llama.yml -f docker-compose.llama-webui-override.yml up -d
# Adds specific configurations for DeepSeek Coder setup

services:
  # Standard container names for llama webui script
  ollama:
    container_name: ollama  # Standard name
    networks:
      - ai_network

  # Add Llama.cpp server
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server  # CPU version by default
    container_name: llama-server
    environment:
      - LLAMA_ARG_MODEL=/models/${LLAMA_MODEL_FILE:-DeepSeek-Coder-V2-Lite-Instruct-Q2_K.gguf}
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=11435
      # CPU-only configuration by default
      - LLAMA_ARG_N_GPU_LAYERS=0
      - LLAMA_ARG_THREADS=8
      - LLAMA_ARG_CTX_SIZE=4096
    volumes:
      - ./models/.llama:/models
    ports:
      - "11435:11435"
    restart: unless-stopped
    networks:
      - ai_network

  # WebUI service configuration override (inherits from base webui file)
  open-webui:
    # Use standard container name for llama webui script
    container_name: open-webui
    environment:
      # Override to connect to standard ollama container
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_NAME=Llama WebUI
    depends_on:
      - ollama
    networks:
      - ai_network

# Use standard volume name for llama webui script
volumes:
  open-webui:

networks:
  ai_network:
    driver: bridge
    external: true
