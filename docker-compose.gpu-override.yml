# Universal GPU Override for Ollama and WebUI Services
# Usage: docker compose -f <base-file> -f docker-compose.gpu-override.yml up -d
# Adds GPU acceleration to Ollama and WebUI services
# For llama-server GPU acceleration, use docker-compose.llama-gpu-override.yml instead

services:
  # Standard Ollama service GPU overrides
  ollama:
    deploy:
      resources:
        limits:
          memory: 22g  # 7B model + GPU overhead + large payload
        reservations:
          memory: 14g   # Just enough for 7B model (more efficient GPU transfers)
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      # RTX 3060 Ti (8GB VRAM) optimized settings for large payloads
      - OLLAMA_FLASH_ATTENTION=true
      # Realistic GPU layer count for 8GB VRAM
      - OLLAMA_NUM_GPU=25  # Optimized for RTX 3060 Ti
      # Large payload GPU optimizations
      - OLLAMA_REQUEST_TIMEOUT=600  # Extended for GPU processing
      - OLLAMA_GPU_MEMORY_FRACTION=0.9
