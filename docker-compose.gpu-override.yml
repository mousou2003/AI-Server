# Universal GPU Override for Ollama and WebUI Services
# Usage: docker compose -f <base-file> -f docker-compose.gpu-override.yml up -d
# Adds GPU acceleration to Ollama and WebUI services
# For llama-server GPU acceleration, use docker-compose.llama-gpu-override.yml instead

services:
  # Standard Ollama service GPU overrides
  ollama:
    deploy:
      resources:
        limits:
          memory: 32g  # Proper allocation for 64GB system with GPU + large payloads
        reservations:
          memory: 20g  # Adequate reservation for 7B model + GPU overhead
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      # RTX 3060 Ti (8GB VRAM) - let Ollama auto-detect optimal layers
      - OLLAMA_FLASH_ATTENTION=true
      # Auto-detect GPU layers (removes manual override)
      # - OLLAMA_NUM_GPU=30  # Commented out for auto-detection
      # Optimized timeouts for GPU + large payload processing
      - OLLAMA_REQUEST_TIMEOUT=600  # Extended for GPU processing
      - OLLAMA_GPU_MEMORY_FRACTION=0.95  # Maximize GPU VRAM usage
