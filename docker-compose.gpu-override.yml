# Universal GPU Override for All Services
# Usage: docker compose -f <base-file> -f docker-compose.gpu-override.yml up -d
# Adds GPU acceleration to any service that supports it
# Works with any container names defined by script-specific overrides

services:
  # Standard Ollama service GPU overrides
  ollama:
    deploy:
      resources:
        limits:
          memory: 32g  # Adjusted for 64GB system RAM
        reservations:
          memory: 8g   # Conservative reservation
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      # RTX 3060 Ti (8GB VRAM) optimized settings
      - OLLAMA_FLASH_ATTENTION=true
      # Realistic GPU layer count for 8GB VRAM
      - OLLAMA_NUM_GPU=25  # Optimized for RTX 3060 Ti

  # Standard Llama.cpp server GPU overrides
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda  # Use CUDA image
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - LLAMA_ARG_N_GPU_LAYERS=35  # Enable GPU layers
      - LLAMA_ARG_NGL=1

  # Standard WebUI GPU overrides
  open-webui:
    image: ghcr.io/open-webui/open-webui:cuda  # Use CUDA image
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
