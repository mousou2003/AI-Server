# Generic Docker Compose Override Template
# This file serves as a template for creating project-specific overrides
# The OllamaCustomModel class will process this template and create
# project-specific override files automatically

services:
  ollama:
    container_name: ollama-churn-assistant
    hostname: ollama-churn-assistant
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
      # Add project-specific environment variables here
    volumes:
      - ./.ollama/churn-assistant:/root/.ollama
    networks:
      - ai_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  open-webui:
    container_name: open-webui-churn-assistant
    hostname: open-webui-churn-assistant
    environment:
      - WEBUI_NAME=Qwen Churn Assistant System Prompt
      - OLLAMA_BASE_URL=http://ollama-churn-assistant:11434
      - WEBUI_AUTH=False
      # Add project-specific WebUI settings here
    volumes:
      - ./.webui/churn-assistant/data:/app/backend/data
      - ./.webui/churn-assistant/workspace:/app/backend/workspace
    networks:
      - ai_network
    depends_on:
      ollama:
        condition: service_healthy

volumes:
  churn-assistant-data:
    driver: local
  churn-assistant-memory:
    driver: local  
  churn-assistant-workspace:
    driver: local

networks:
  ai_network:
    external: true