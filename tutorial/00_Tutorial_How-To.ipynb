{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65c02ea",
   "metadata": {},
   "source": [
    "# Ollama Environment Setup\n",
    "\n",
    "This cell sets up the Python environment and connects to your local Ollama server.\n",
    "It installs required packages, sets model and host variables, and prepares the Ollama client for use.\n",
    "\n",
    "**Purpose:**\n",
    "- Ensures all dependencies are installed and variables are set.\n",
    "- Initializes the Ollama client so you can send prompts to the model.\n",
    "- Used as a setup step in other tutorial notebooks via `%run 00_Tutorial_How-To.ipynb`.\n",
    "- Run this cell first before using any prompt engineering or model interaction cells.\n",
    "\n",
    "Refer to the project [README](../readme.md) for instructions on installing the local Ollama server.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Notes & Tips ðŸ’¡\n",
    "- This tutorial uses Qwen 2.5 7B Instruct with temperature 0. You can change the model name to any available in the [Ollama model library](https://ollama.com/search).\n",
    "- Use `Shift + Enter` to execute the cell and move to the next one.\n",
    "\n",
    "### The Ollama Python Library\n",
    "We will be using the [Ollama Python Library](https://github.com/ollama/ollama-python) throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabdc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install -U ollama tqdm pickleshare nbformat --quiet\n",
    "%pip install python-dotenv --quiet\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This loads variables from .env into the environment\n",
    "\n",
    "# Now you can use OLLAMA_HOST from the .env file\n",
    "OLLAMA_HOST = os.getenv('OLLAMA_HOST')\n",
    "print(f'Using Ollama host: {OLLAMA_HOST}')\n",
    "\n",
    "# Set up your model name\n",
    "MODEL_NAME = 'qwen2.5:14b-instruct'\n",
    "# Stores the MODEL_NAME variable for use across notebooks within the IPython store\n",
    "%store MODEL_NAME\n",
    "\n",
    "# Connect to Ollama Service\n",
    "from ollama import Client\n",
    "if 'client' not in globals():\n",
    "    client = Client(host=OLLAMA_HOST)\n",
    "    print(f'Connected to Ollama at {OLLAMA_HOST}')\n",
    "    print(f'Using model: {MODEL_NAME}')\n",
    "else:\n",
    "    print(\"Ollama client already initialized.\")\n",
    "\n",
    "# Helper function to send a prompt or conversation to Ollama and get the response\n",
    "def get_completion(prompt_or_messages, system_prompt=\"\", max_tokens=2000, temperature=0.0):\n",
    "    from ollama import Options\n",
    "    if isinstance(prompt_or_messages, str):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_or_messages}\n",
    "        ]\n",
    "    else:\n",
    "        # If system_prompt is provided, ensure it is the first message\n",
    "        messages = prompt_or_messages\n",
    "        if system_prompt:\n",
    "            # Remove any existing system message to avoid duplicates\n",
    "            messages = [m for m in messages if m.get(\"role\") != \"system\"]\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    response = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        options=Options(max_tokens=max_tokens, temperature=temperature),\n",
    "        messages=messages\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Pretty print function for Ollama response objects\n",
    "def pretty_print_response(response):\n",
    "    from pprint import pprint  # For pretty-printing fallback\n",
    "    from datetime import datetime  # For friendly timestamp formatting\n",
    "    def ns_to_sec(ns):  # Convert nanoseconds to seconds for readability\n",
    "        return f\"{ns/1e9:.2f} s\" if ns is not None else None\n",
    "    # Print model name used for the response\n",
    "    print(\"Model:\", getattr(response, 'model', None))\n",
    "    # Print timestamp when the response was created, formatted for readability\n",
    "    created_at = getattr(response, 'created_at', None)\n",
    "    if created_at:\n",
    "        try:\n",
    "            # Try to parse and format ISO 8601 timestamp\n",
    "            dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))\n",
    "            print(\"Created at:\", dt.strftime('%Y-%m-%d %H:%M:%S UTC'))\n",
    "        except Exception:\n",
    "            print(\"Created at:\", created_at)\n",
    "    else:\n",
    "        print(\"Created at:\", created_at)\n",
    "    # Print whether the response is finished\n",
    "    print(\"Done:\", getattr(response, 'done', None))\n",
    "    # Print the reason why the response finished\n",
    "    print(\"Done reason:\", getattr(response, 'done_reason', None))\n",
    "    # Print total duration in seconds\n",
    "    print(\"Total duration:\", ns_to_sec(getattr(response, 'total_duration', None)))\n",
    "    # Print model load duration in seconds\n",
    "    print(\"Load duration:\", ns_to_sec(getattr(response, 'load_duration', None)))\n",
    "    # Print number of prompt tokens evaluated\n",
    "    print(\"Prompt eval count:\", getattr(response, 'prompt_eval_count', None))\n",
    "    # Print prompt evaluation duration in seconds\n",
    "    print(\"Prompt eval duration:\", ns_to_sec(getattr(response, 'prompt_eval_duration', None)))\n",
    "    # Print number of tokens evaluated in response\n",
    "    print(\"Eval count:\", getattr(response, 'eval_count', None))\n",
    "    # Print evaluation duration in seconds\n",
    "    # 'Evaluation' here means the process of generating tokens for the model's output (the response)\n",
    "    print(\"Eval duration:\", ns_to_sec(getattr(response, 'eval_duration', None)))\n",
    "    # Print the message content from the assistant\n",
    "    print(\"Message:\")\n",
    "    msg = getattr(response, 'message', None)\n",
    "    if msg:\n",
    "        # Print the role of the message (e.g., 'assistant')\n",
    "        print(\"  Role:\", getattr(msg, 'role', None))\n",
    "        # Print the actual content of the message\n",
    "        print(\"  Content:\", getattr(msg, 'content', None))\n",
    "    else:\n",
    "        # Fallback: pretty-print the whole response object if no message found\n",
    "        pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
