# Open WebUI - Base Configuration (CPU-optimized by default)
# Can connect to any Ollama or compatible API endpoint
# Includes user management, memory, and tool features
# Use docker-compose.gpu-override.yml for GPU acceleration

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main  # CPU-friendly image
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - WEBUI_NAME=${WEBUI_NAME:-Open WebUI}
      # User access configuration
      - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE:-admin}
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true}
      # Feature enablement
      - ENABLE_MEMORY=${ENABLE_MEMORY:-true}
      - ENABLE_TOOLS=${ENABLE_TOOLS:-true}
      - TOOLS_FUNCTION_CALLING_ENABLED=${TOOLS_FUNCTION_CALLING_ENABLED:-true}
      # Large payload optimizations
      - WEBUI_REQUEST_TIMEOUT=300
      - WEBUI_MAX_FILE_SIZE=50m
      - OLLAMA_REQUEST_TIMEOUT=300
    volumes:
      - open-webui:/app/backend/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health", "||", "curl", "-f", "http://localhost:8080/", "||", "exit", "1"]
      interval: 60s   # Longer interval for large processing
      timeout: 30s    # Increased timeout
      retries: 5
      start_period: 180s  # Give WebUI 3 minutes to fully initialize
    deploy:
      resources:
        limits:
          memory: ${WEBUI_MEM_LIMIT:-6g}  # Sufficient for WebUI + large payload handling
        reservations:
          memory: ${WEBUI_MEM_RESERVATION:-3g}  # Adequate reservation
    networks:
      - ai_network

volumes:
  open-webui:

networks:
  ai_network:
    driver: bridge
    external: true
