# Open WebUI - Base Configuration (CPU-optimized by default)
# Can connect to any Ollama or compatible API endpoint
# Includes user management, memory, and tool features
# Use docker-compose.gpu-override.yml for GPU acceleration

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main  # CPU-friendly image
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - WEBUI_NAME=${WEBUI_NAME:-Open WebUI}
      # User access configuration
      - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE:-admin}
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true}
      # Feature enablement
      - ENABLE_MEMORY=${ENABLE_MEMORY:-true}
      - ENABLE_TOOLS=${ENABLE_TOOLS:-true}
      - TOOLS_FUNCTION_CALLING_ENABLED=${TOOLS_FUNCTION_CALLING_ENABLED:-true}
      # Standard configuration for laptop use (supports up to 20MB payloads)
      - WEBUI_REQUEST_TIMEOUT=120      # 2 minutes for normal requests
      - WEBUI_MAX_FILE_SIZE=20m        # 20MB limit (covers 16MB + overhead)
      - OLLAMA_REQUEST_TIMEOUT=120     # Match WebUI timeout
    volumes:
      - open-webui:/app/backend/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health", "||", "curl", "-f", "http://localhost:8080/", "||", "exit", "1"]
      interval: 30s   # Standard interval
      timeout: 10s    # Standard timeout
      retries: 3
      start_period: 120s  # Standard startup time
    deploy:
      resources:
        limits:
          memory: ${WEBUI_MEM_LIMIT:-3g}  # Reduced for memory-constrained laptop
        reservations:
          memory: ${WEBUI_MEM_RESERVATION:-1g}  # Minimal reservation
    networks:
      - ai_network

volumes:
  open-webui:

networks:
  ai_network:
    driver: bridge
    external: true
